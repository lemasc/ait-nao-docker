{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PostgreSQL B-tree Index Performance Analysis\n",
    "## Disk-Resident Dataset (16M rows) - 3 Round Results\n",
    "\n",
    "This notebook analyzes the performance tradeoffs between no indexing vs single-column B-tree indexing across different OLTP read/write workloads.\n",
    "\n",
    "**Experimental Setup:**\n",
    "- Dataset: 16M rows (~2Ã— RAM, disk-resident)\n",
    "- PostgreSQL 15 with 4 CPUs, 8GB RAM\n",
    "- Indexing: No index vs B-tree on `indexed_col`\n",
    "- Read/Write Ratios: 90/10, 50/50, 10/90\n",
    "- Concurrency Levels: 1, 8, 16, 32 clients\n",
    "- 3 rounds for statistical confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Configure plot defaults\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Create directory for plot exports\n",
    "PLOTS_DIR = Path('analysis_outputs/plots')\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOTS_RESOLUTION = 96 # Shift this value when generating reports\n",
    "print(f\"Plot exports will be saved to: {PLOTS_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_results(base_path='results/disk-resident'):\n",
    "    \"\"\"\n",
    "    Load all JSON results from 3 rounds.\n",
    "    Returns a structured DataFrame with configuration and metrics.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    for round_num in [1, 2, 3]:\n",
    "        round_path = Path(base_path) / str(round_num) / 'results'\n",
    "        json_files = list(round_path.glob('*.json'))\n",
    "        \n",
    "        # Filter out copy files\n",
    "        json_files = [f for f in json_files if 'copy' not in f.name]\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Extract configuration\n",
    "            config = data['config']\n",
    "            read_ratio, write_ratio = config['read_write_ratio']\n",
    "            \n",
    "            # Create base record\n",
    "            base_record = {\n",
    "                'round': round_num,\n",
    "                'indexed': config['indexed'],\n",
    "                'index_label': 'Indexed' if config['indexed'] else 'No Index',\n",
    "                'read_ratio': read_ratio,\n",
    "                'write_ratio': write_ratio,\n",
    "                'ratio_label': f\"{read_ratio}/{write_ratio}\",\n",
    "                'concurrency': config['concurrency'],\n",
    "                'duration_seconds': data['duration_seconds'],\n",
    "                'total_operations': data['total_operations'],\n",
    "                'total_ops_per_sec': data['operations_per_second']\n",
    "            }\n",
    "            \n",
    "            # Extract per-operation metrics\n",
    "            for op_type, metrics in data['summary'].items():\n",
    "                record = base_record.copy()\n",
    "                record.update({\n",
    "                    'operation': op_type,\n",
    "                    'count': metrics['count'],\n",
    "                    'success': metrics['success'],\n",
    "                    'error': metrics['error'],\n",
    "                    'min_latency_ms': metrics['min_latency_ms'],\n",
    "                    'max_latency_ms': metrics['max_latency_ms'],\n",
    "                    'mean_latency_ms': metrics['mean_latency_ms'],\n",
    "                    'p50_latency_ms': metrics['p50_latency_ms'],\n",
    "                    'p95_latency_ms': metrics['p95_latency_ms'],\n",
    "                    'p99_latency_ms': metrics['p99_latency_ms'],\n",
    "                    'ops_per_sec': metrics['ops_per_sec'],\n",
    "                    'timeout_statement': metrics['errors_by_type']['timeout_statement'],\n",
    "                    'timeout_lock': metrics['errors_by_type']['timeout_lock'],\n",
    "                    'deadlock': metrics['errors_by_type']['deadlock'],\n",
    "                    'error_other': metrics['errors_by_type']['other']\n",
    "                })\n",
    "                all_data.append(record)\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    # Add operation categories\n",
    "    df['op_category'] = df['operation'].apply(\n",
    "        lambda x: 'read' if x in ['point_lookup', 'range_scan', 'range_order'] else 'write'\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load all data\n",
    "df = load_all_results()\n",
    "\n",
    "print(f\"Loaded {len(df)} records\")\n",
    "print(f\"Rounds: {sorted(df['round'].unique())}\")\n",
    "print(f\"Configurations: {len(df.groupby(['indexed', 'ratio_label', 'concurrency', 'operation']))}\")\n",
    "print(f\"Operations: {sorted(df['operation'].unique())}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute aggregated statistics across rounds\n",
    "def compute_statistics(df):\n",
    "    \"\"\"\n",
    "    Aggregate metrics across 3 rounds with mean and 95% confidence intervals.\n",
    "    \"\"\"\n",
    "    group_cols = ['indexed', 'index_label', 'read_ratio', 'write_ratio', 'ratio_label',\n",
    "                  'concurrency', 'operation', 'op_category']\n",
    "\n",
    "    # Define aggregation functions separately for compatibility\n",
    "    def calc_ci_95(series):\n",
    "        \"\"\"Calculate 95% confidence interval using t-distribution\"\"\"\n",
    "        n = len(series)\n",
    "        if n > 1:\n",
    "            std = series.std()\n",
    "            return stats.t.ppf(0.975, n-1) * std / np.sqrt(n)\n",
    "        return 0\n",
    "\n",
    "    # Aggregate with standard functions\n",
    "    agg_df = df.groupby(group_cols).agg({\n",
    "        'p95_latency_ms': ['mean', 'std', 'min', 'max', calc_ci_95],\n",
    "        'p50_latency_ms': ['mean', 'std', 'min', 'max', calc_ci_95],\n",
    "        'mean_latency_ms': ['mean', 'std', 'min', 'max', calc_ci_95],\n",
    "        'ops_per_sec': ['mean', 'std', 'min', 'max', calc_ci_95],\n",
    "        'total_ops_per_sec': ['mean', 'std', 'min', 'max', calc_ci_95],\n",
    "        'count': ['mean', 'std', 'min', 'max', calc_ci_95],\n",
    "        'error': 'sum',\n",
    "        'timeout_statement': 'sum',\n",
    "        'timeout_lock': 'sum',\n",
    "        'deadlock': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten multi-level columns with proper naming\n",
    "    new_columns = []\n",
    "    for col in agg_df.columns:\n",
    "        if isinstance(col, tuple):\n",
    "            if col[1] == 'calc_ci_95':\n",
    "                new_columns.append(f\"{col[0]}_ci_95\")\n",
    "            elif col[1] in ['mean', 'std', 'min', 'max']:\n",
    "                new_columns.append(f\"{col[0]}_{col[1]}\")\n",
    "            else:\n",
    "                new_columns.append('_'.join(str(c) for c in col).strip('_'))\n",
    "        else:\n",
    "            new_columns.append(col)\n",
    "\n",
    "    agg_df.columns = new_columns\n",
    "\n",
    "    return agg_df\n",
    "\n",
    "stats_df = compute_statistics(df)\n",
    "print(f\"\\nAggregated statistics: {len(stats_df)} configurations\")\n",
    "stats_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for errors and timeouts\n",
    "error_summary = df.groupby(['indexed', 'ratio_label', 'concurrency', 'operation']).agg({\n",
    "    'error': 'sum',\n",
    "    'timeout_statement': 'sum',\n",
    "    'timeout_lock': 'sum',\n",
    "    'deadlock': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "total_errors = error_summary['error'].sum()\n",
    "total_timeouts = error_summary['timeout_statement'].sum() + error_summary['timeout_lock'].sum()\n",
    "\n",
    "print(f\"Total errors across all runs: {total_errors}\")\n",
    "print(f\"Total timeouts: {total_timeouts}\")\n",
    "\n",
    "if total_errors > 0 or total_timeouts > 0:\n",
    "    print(\"\\nConfigurations with errors/timeouts:\")\n",
    "    problematic = error_summary[\n",
    "        (error_summary['error'] > 0) | \n",
    "        (error_summary['timeout_statement'] > 0) | \n",
    "        (error_summary['timeout_lock'] > 0)\n",
    "    ]\n",
    "    display(problematic)\n",
    "else:\n",
    "    print(\"âœ“ No errors or timeouts detected - clean dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check variability across rounds\n",
    "variability = stats_df[['indexed', 'ratio_label', 'concurrency', 'operation', \n",
    "                        'p95_latency_ms_mean', 'p95_latency_ms_std', 'p95_latency_ms_ci_95']].copy()\n",
    "variability['cv'] = (variability['p95_latency_ms_std'] / variability['p95_latency_ms_mean'] * 100).round(2)\n",
    "variability = variability.sort_values('cv', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 most variable configurations (by coefficient of variation):\")\n",
    "display(variability.head(10))\n",
    "\n",
    "print(f\"\\nMedian CV: {variability['cv'].median():.2f}%\")\n",
    "print(f\"Mean CV: {variability['cv'].mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. P95 Latency Analysis\n",
    "\n",
    "Primary decision-driving metric: **P95 latency** per operation type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_p95_latency_by_ratio(stats_df, operation_filter=None, figsize=(16, 10)):\n",
    "    \"\"\"\n",
    "    Plot P95 latency vs concurrency for each read/write ratio.\n",
    "    Separate plots for each operation type.\n",
    "    \"\"\"\n",
    "    df_plot = stats_df.copy()\n",
    "    \n",
    "    if operation_filter:\n",
    "        df_plot = df_plot[df_plot['operation'].isin(operation_filter)]\n",
    "    \n",
    "    operations = sorted(df_plot['operation'].unique())\n",
    "    ratios = sorted(df_plot['ratio_label'].unique(), key=lambda x: int(x.split('/')[0]), reverse=True)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(operations), len(ratios), figsize=figsize, squeeze=False)\n",
    "    fig.suptitle('P95 Latency vs Concurrency (with 95% CI)', fontsize=16, y=1.0)\n",
    "    \n",
    "    for op_idx, operation in enumerate(operations):\n",
    "        for ratio_idx, ratio in enumerate(ratios):\n",
    "            ax = axes[op_idx, ratio_idx]\n",
    "            \n",
    "            data = df_plot[(df_plot['operation'] == operation) & (df_plot['ratio_label'] == ratio)]\n",
    "            \n",
    "            for indexed in [True, False]:\n",
    "                subset = data[data['indexed'] == indexed].sort_values('concurrency')\n",
    "                \n",
    "                if len(subset) > 0:\n",
    "                    label = 'Indexed' if indexed else 'No Index'\n",
    "                    color = 'blue' if indexed else 'red'\n",
    "                    \n",
    "                    ax.errorbar(\n",
    "                        subset['concurrency'], \n",
    "                        subset['p95_latency_ms_mean'],\n",
    "                        yerr=subset['p95_latency_ms_ci_95'],\n",
    "                        marker='o', \n",
    "                        label=label,\n",
    "                        color=color,\n",
    "                        capsize=5,\n",
    "                        linewidth=2,\n",
    "                        markersize=8\n",
    "                    )\n",
    "            \n",
    "            # Formatting\n",
    "            if op_idx == 0:\n",
    "                ax.set_title(f'Read/Write: {ratio}', fontsize=12, fontweight='bold')\n",
    "            if ratio_idx == 0:\n",
    "                ax.set_ylabel(f'{operation}\\nP95 Latency (ms)', fontsize=11)\n",
    "            if op_idx == len(operations) - 1:\n",
    "                ax.set_xlabel('Concurrency', fontsize=11)\n",
    "            \n",
    "            ax.set_xticks([1, 8, 16, 32])\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend(loc='best', fontsize=9)\n",
    "            \n",
    "            # Use log scale for y-axis if range is large\n",
    "            if data['p95_latency_ms_mean'].max() / data['p95_latency_ms_mean'].min() > 100:\n",
    "                ax.set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Plot all operations\n",
    "fig = plot_p95_latency_by_ratio(stats_df)\n",
    "fig.savefig(PLOTS_DIR / '01_p95_latency_all_operations.png', dpi=PLOTS_RESOLUTION, bbox_inches='tight')\n",
    "print(f\"Saved: {PLOTS_DIR / '01_p95_latency_all_operations.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focused view: Read operations only\n",
    "fig = plot_p95_latency_by_ratio(\n",
    "    stats_df, \n",
    "    operation_filter=['point_lookup', 'range_scan', 'range_order'],\n",
    "    figsize=(16, 8)\n",
    ")\n",
    "fig.savefig(PLOTS_DIR / '02_p95_latency_read_operations.png', dpi=PLOTS_RESOLUTION, bbox_inches='tight')\n",
    "print(f\"Saved: {PLOTS_DIR / '02_p95_latency_read_operations.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focused view: Write operations only\n",
    "fig = plot_p95_latency_by_ratio(\n",
    "    stats_df, \n",
    "    operation_filter=['insert', 'update'],\n",
    "    figsize=(16, 5)\n",
    ")\n",
    "fig.savefig(PLOTS_DIR / '03_p95_latency_write_operations.png', dpi=PLOTS_RESOLUTION, bbox_inches='tight')\n",
    "print(f\"Saved: {PLOTS_DIR / '03_p95_latency_write_operations.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Throughput Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_throughput_by_ratio(stats_df, figsize=(16, 5)):\n",
    "    \"\"\"\n",
    "    Plot total throughput (ops/sec) vs concurrency for each ratio.\n",
    "    \"\"\"\n",
    "    # Get total throughput (aggregate across operations for same config)\n",
    "    throughput_df = stats_df.groupby(['indexed', 'index_label', 'ratio_label', 'concurrency']).agg({\n",
    "        'total_ops_per_sec_mean': 'first',  # Already aggregated\n",
    "        'total_ops_per_sec_ci_95': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    ratios = sorted(throughput_df['ratio_label'].unique(), key=lambda x: int(x.split('/')[0]), reverse=True)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(ratios), figsize=figsize, squeeze=False)\n",
    "    fig.suptitle('Total Throughput vs Concurrency (with 95% CI)', fontsize=16)\n",
    "    \n",
    "    for ratio_idx, ratio in enumerate(ratios):\n",
    "        ax = axes[0, ratio_idx]\n",
    "        data = throughput_df[throughput_df['ratio_label'] == ratio]\n",
    "        \n",
    "        for indexed in [True, False]:\n",
    "            subset = data[data['indexed'] == indexed].sort_values('concurrency')\n",
    "            \n",
    "            if len(subset) > 0:\n",
    "                label = 'Indexed' if indexed else 'No Index'\n",
    "                color = 'blue' if indexed else 'red'\n",
    "                \n",
    "                ax.errorbar(\n",
    "                    subset['concurrency'], \n",
    "                    subset['total_ops_per_sec_mean'],\n",
    "                    yerr=subset['total_ops_per_sec_ci_95'],\n",
    "                    marker='o', \n",
    "                    label=label,\n",
    "                    color=color,\n",
    "                    capsize=5,\n",
    "                    linewidth=2,\n",
    "                    markersize=8\n",
    "                )\n",
    "        \n",
    "        ax.set_title(f'Read/Write: {ratio}', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Concurrency', fontsize=11)\n",
    "        if ratio_idx == 0:\n",
    "            ax.set_ylabel('Total Throughput (ops/sec)', fontsize=11)\n",
    "        ax.set_xticks([1, 8, 16, 32])\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(loc='best', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "fig = plot_throughput_by_ratio(stats_df)\n",
    "fig.savefig(PLOTS_DIR / '04_total_throughput.png', dpi=PLOTS_RESOLUTION, bbox_inches='tight')\n",
    "print(f\"Saved: {PLOTS_DIR / '04_total_throughput.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-operation throughput comparison\n",
    "def plot_per_operation_throughput(stats_df, figsize=(16, 10)):\n",
    "    \"\"\"\n",
    "    Plot per-operation throughput vs concurrency.\n",
    "    \"\"\"\n",
    "    operations = sorted(stats_df['operation'].unique())\n",
    "    ratios = sorted(stats_df['ratio_label'].unique(), key=lambda x: int(x.split('/')[0]), reverse=True)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(operations), len(ratios), figsize=figsize, squeeze=False)\n",
    "    fig.suptitle('Per-Operation Throughput vs Concurrency', fontsize=16, y=1.0)\n",
    "    \n",
    "    for op_idx, operation in enumerate(operations):\n",
    "        for ratio_idx, ratio in enumerate(ratios):\n",
    "            ax = axes[op_idx, ratio_idx]\n",
    "            \n",
    "            data = stats_df[(stats_df['operation'] == operation) & (stats_df['ratio_label'] == ratio)]\n",
    "            \n",
    "            for indexed in [True, False]:\n",
    "                subset = data[data['indexed'] == indexed].sort_values('concurrency')\n",
    "                \n",
    "                if len(subset) > 0:\n",
    "                    label = 'Indexed' if indexed else 'No Index'\n",
    "                    color = 'blue' if indexed else 'red'\n",
    "                    \n",
    "                    ax.errorbar(\n",
    "                        subset['concurrency'], \n",
    "                        subset['ops_per_sec_mean'],\n",
    "                        yerr=subset['ops_per_sec_ci_95'],\n",
    "                        marker='o', \n",
    "                        label=label,\n",
    "                        color=color,\n",
    "                        capsize=5,\n",
    "                        linewidth=2,\n",
    "                        markersize=8\n",
    "                    )\n",
    "            \n",
    "            if op_idx == 0:\n",
    "                ax.set_title(f'{ratio}', fontsize=11, fontweight='bold')\n",
    "            if ratio_idx == 0:\n",
    "                ax.set_ylabel(f'{operation}\\n(ops/sec)', fontsize=10)\n",
    "            if op_idx == len(operations) - 1:\n",
    "                ax.set_xlabel('Concurrency', fontsize=10)\n",
    "            \n",
    "            ax.set_xticks([1, 8, 16, 32])\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend(loc='best', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "fig = plot_per_operation_throughput(stats_df)\n",
    "fig.savefig(PLOTS_DIR / '05_per_operation_throughput.png', dpi=PLOTS_RESOLUTION, bbox_inches='tight')\n",
    "print(f\"Saved: {PLOTS_DIR / '05_per_operation_throughput.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Index Overhead Analysis\n",
    "\n",
    "Quantify the write penalty introduced by maintaining the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate index overhead for writes\n",
    "def calculate_index_overhead(stats_df):\n",
    "    \"\"\"\n",
    "    Calculate relative overhead: (indexed - no_index) / no_index * 100%\n",
    "    \"\"\"\n",
    "    overhead_data = []\n",
    "    \n",
    "    for ratio in stats_df['ratio_label'].unique():\n",
    "        for concurrency in stats_df['concurrency'].unique():\n",
    "            for operation in stats_df['operation'].unique():\n",
    "                subset = stats_df[\n",
    "                    (stats_df['ratio_label'] == ratio) &\n",
    "                    (stats_df['concurrency'] == concurrency) &\n",
    "                    (stats_df['operation'] == operation)\n",
    "                ]\n",
    "                \n",
    "                if len(subset) == 2:  # Both indexed and no_index present\n",
    "                    indexed_row = subset[subset['indexed'] == True].iloc[0]\n",
    "                    no_index_row = subset[subset['indexed'] == False].iloc[0]\n",
    "                    \n",
    "                    # Calculate overhead for p95 latency\n",
    "                    p95_overhead = (\n",
    "                        (indexed_row['p95_latency_ms_mean'] - no_index_row['p95_latency_ms_mean']) \n",
    "                        / no_index_row['p95_latency_ms_mean'] * 100\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate overhead for throughput (negative = improvement)\n",
    "                    throughput_overhead = (\n",
    "                        (indexed_row['ops_per_sec_mean'] - no_index_row['ops_per_sec_mean'])\n",
    "                        / no_index_row['ops_per_sec_mean'] * 100\n",
    "                    )\n",
    "                    \n",
    "                    overhead_data.append({\n",
    "                        'ratio_label': ratio,\n",
    "                        'concurrency': concurrency,\n",
    "                        'operation': operation,\n",
    "                        'op_category': indexed_row['op_category'],\n",
    "                        'p95_overhead_pct': p95_overhead,\n",
    "                        'throughput_overhead_pct': throughput_overhead,\n",
    "                        'indexed_p95': indexed_row['p95_latency_ms_mean'],\n",
    "                        'no_index_p95': no_index_row['p95_latency_ms_mean'],\n",
    "                        'indexed_throughput': indexed_row['ops_per_sec_mean'],\n",
    "                        'no_index_throughput': no_index_row['ops_per_sec_mean']\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(overhead_data)\n",
    "\n",
    "overhead_df = calculate_index_overhead(stats_df)\n",
    "\n",
    "print(\"Index Overhead Summary (negative = index is better):\")\n",
    "print(\"\\nBy Operation Type:\")\n",
    "display(overhead_df.groupby('operation')[['p95_overhead_pct', 'throughput_overhead_pct']].mean().round(2))\n",
    "\n",
    "print(\"\\nBy Read/Write Category:\")\n",
    "display(overhead_df.groupby('op_category')[['p95_overhead_pct', 'throughput_overhead_pct']].mean().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot write operation overhead\n",
    "def plot_write_overhead(overhead_df, figsize=(14, 5)):\n",
    "    \"\"\"\n",
    "    Plot write operation overhead across configurations.\n",
    "    \"\"\"\n",
    "    write_ops = overhead_df[overhead_df['op_category'] == 'write']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "    fig.suptitle('Write Operation Overhead: Index vs No Index', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # P95 Latency Overhead\n",
    "    ax = axes[0]\n",
    "    for operation in write_ops['operation'].unique():\n",
    "        op_data = write_ops[write_ops['operation'] == operation]\n",
    "        for ratio in sorted(op_data['ratio_label'].unique()):\n",
    "            subset = op_data[op_data['ratio_label'] == ratio].sort_values('concurrency')\n",
    "            ax.plot(\n",
    "                subset['concurrency'],\n",
    "                subset['p95_overhead_pct'],\n",
    "                marker='o',\n",
    "                label=f'{operation} ({ratio})',\n",
    "                linewidth=2,\n",
    "                markersize=6\n",
    "            )\n",
    "    \n",
    "    ax.axhline(y=0, color='black', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    ax.set_xlabel('Concurrency', fontsize=11)\n",
    "    ax.set_ylabel('P95 Latency Overhead (%)', fontsize=11)\n",
    "    ax.set_title('P95 Latency Impact', fontsize=12)\n",
    "    ax.set_xticks([1, 8, 16, 32])\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=9, loc='best')\n",
    "    \n",
    "    # Throughput Overhead\n",
    "    ax = axes[1]\n",
    "    for operation in write_ops['operation'].unique():\n",
    "        op_data = write_ops[write_ops['operation'] == operation]\n",
    "        for ratio in sorted(op_data['ratio_label'].unique()):\n",
    "            subset = op_data[op_data['ratio_label'] == ratio].sort_values('concurrency')\n",
    "            ax.plot(\n",
    "                subset['concurrency'],\n",
    "                subset['throughput_overhead_pct'],\n",
    "                marker='o',\n",
    "                label=f'{operation} ({ratio})',\n",
    "                linewidth=2,\n",
    "                markersize=6\n",
    "            )\n",
    "    \n",
    "    ax.axhline(y=0, color='black', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    ax.set_xlabel('Concurrency', fontsize=11)\n",
    "    ax.set_ylabel('Throughput Overhead (%)', fontsize=11)\n",
    "    ax.set_title('Throughput Impact', fontsize=12)\n",
    "    ax.set_xticks([1, 8, 16, 32])\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=9, loc='best')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "fig = plot_write_overhead(overhead_df)\n",
    "fig.savefig(PLOTS_DIR / '06_write_overhead.png', dpi=PLOTS_RESOLUTION, bbox_inches='tight')\n",
    "print(f\"Saved: {PLOTS_DIR / '06_write_overhead.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Heatmap: Index Wins vs Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_index_benefit_heatmap(overhead_df, metric='p95_overhead_pct', figsize=(14, 8)):\n",
    "    \"\"\"\n",
    "    Heatmap showing index benefit (negative = index wins) across ratio Ã— concurrency.\n",
    "    \"\"\"\n",
    "    operations = sorted(overhead_df['operation'].unique())\n",
    "    n_ops = len(operations)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_ops, figsize=figsize)\n",
    "    if n_ops == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    title_map = {\n",
    "        'p95_overhead_pct': 'P95 Latency Overhead (%)',\n",
    "        'throughput_overhead_pct': 'Throughput Change (%)'\n",
    "    }\n",
    "    \n",
    "    fig.suptitle(f'Index Impact Heatmap: {title_map.get(metric, metric)}\\n(Negative = Index Better)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    for idx, operation in enumerate(operations):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Pivot data for heatmap\n",
    "        op_data = overhead_df[overhead_df['operation'] == operation]\n",
    "        pivot = op_data.pivot_table(\n",
    "            values=metric,\n",
    "            index='ratio_label',\n",
    "            columns='concurrency',\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        # Sort by read ratio descending\n",
    "        pivot = pivot.sort_index(key=lambda x: x.map(lambda s: int(s.split('/')[0])), ascending=False)\n",
    "        \n",
    "        # Create heatmap\n",
    "        vmax = max(abs(pivot.min().min()), abs(pivot.max().max()))\n",
    "        sns.heatmap(\n",
    "            pivot,\n",
    "            annot=True,\n",
    "            fmt='.1f',\n",
    "            cmap='RdYlGn_r',  # Red = bad (higher overhead), Green = good (lower overhead)\n",
    "            center=0,\n",
    "            vmin=-vmax,\n",
    "            vmax=vmax,\n",
    "            ax=ax,\n",
    "            cbar_kws={'label': '%'},\n",
    "            linewidths=0.5\n",
    "        )\n",
    "        \n",
    "        ax.set_title(f'{operation}', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Concurrency', fontsize=11)\n",
    "        if idx == 0:\n",
    "            ax.set_ylabel('Read/Write Ratio', fontsize=11)\n",
    "        else:\n",
    "            ax.set_ylabel('')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# P95 latency overhead heatmap\n",
    "fig = plot_index_benefit_heatmap(overhead_df, metric='p95_overhead_pct')\n",
    "fig.savefig(PLOTS_DIR / '07_heatmap_p95_overhead.png', dpi=PLOTS_RESOLUTION, bbox_inches='tight')\n",
    "print(f\"Saved: {PLOTS_DIR / '07_heatmap_p95_overhead.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throughput overhead heatmap\n",
    "fig = plot_index_benefit_heatmap(overhead_df, metric='throughput_overhead_pct')\n",
    "fig.savefig(PLOTS_DIR / '08_heatmap_throughput_overhead.png', dpi=PLOTS_RESOLUTION, bbox_inches='tight')\n",
    "print(f\"Saved: {PLOTS_DIR / '08_heatmap_throughput_overhead.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Concurrency Scaling Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze scalability: throughput increase from c1 to c32\n",
    "def analyze_scalability(stats_df):\n",
    "    \"\"\"\n",
    "    Calculate scalability factor: throughput@c32 / throughput@c1\n",
    "    \"\"\"\n",
    "    scalability_data = []\n",
    "    \n",
    "    for indexed in [True, False]:\n",
    "        for ratio in stats_df['ratio_label'].unique():\n",
    "            for operation in stats_df['operation'].unique():\n",
    "                subset = stats_df[\n",
    "                    (stats_df['indexed'] == indexed) &\n",
    "                    (stats_df['ratio_label'] == ratio) &\n",
    "                    (stats_df['operation'] == operation)\n",
    "                ]\n",
    "                \n",
    "                c1 = subset[subset['concurrency'] == 1]\n",
    "                c32 = subset[subset['concurrency'] == 32]\n",
    "                \n",
    "                if len(c1) > 0 and len(c32) > 0:\n",
    "                    scalability_factor = c32.iloc[0]['ops_per_sec_mean'] / c1.iloc[0]['ops_per_sec_mean']\n",
    "                    \n",
    "                    scalability_data.append({\n",
    "                        'indexed': indexed,\n",
    "                        'index_label': 'Indexed' if indexed else 'No Index',\n",
    "                        'ratio_label': ratio,\n",
    "                        'operation': operation,\n",
    "                        'scalability_factor': scalability_factor,\n",
    "                        'throughput_c1': c1.iloc[0]['ops_per_sec_mean'],\n",
    "                        'throughput_c32': c32.iloc[0]['ops_per_sec_mean']\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(scalability_data)\n",
    "\n",
    "scalability_df = analyze_scalability(stats_df)\n",
    "\n",
    "print(\"Scalability Analysis (Throughput @ c32 / Throughput @ c1):\")\n",
    "print(\"\\nBy Operation and Index:\")\n",
    "display(\n",
    "    scalability_df.pivot_table(\n",
    "        values='scalability_factor',\n",
    "        index='operation',\n",
    "        columns='index_label',\n",
    "        aggfunc='mean'\n",
    "    ).round(2)\n",
    ")\n",
    "\n",
    "print(\"\\nDetailed Scalability by Configuration:\")\n",
    "display(scalability_df.sort_values(['operation', 'ratio_label', 'indexed']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics for key findings\n",
    "print(\"=\"*80)\n",
    "print(\"KEY FINDINGS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. READ OPERATION PERFORMANCE (P95 Latency)\")\n",
    "print(\"-\" * 80)\n",
    "read_ops = stats_df[stats_df['op_category'] == 'read']\n",
    "for operation in ['point_lookup', 'range_scan', 'range_order']:\n",
    "    op_data = read_ops[read_ops['operation'] == operation]\n",
    "    indexed = op_data[op_data['indexed'] == True]['p95_latency_ms_mean'].mean()\n",
    "    no_index = op_data[op_data['indexed'] == False]['p95_latency_ms_mean'].mean()\n",
    "    improvement = (no_index - indexed) / no_index * 100\n",
    "    \n",
    "    print(f\"\\n{operation}:\")\n",
    "    print(f\"  Indexed avg P95:    {indexed:.2f} ms\")\n",
    "    print(f\"  No Index avg P95:   {no_index:.2f} ms\")\n",
    "    print(f\"  Index improvement:  {improvement:+.1f}%\")\n",
    "\n",
    "print(\"\\n\\n2. WRITE OPERATION PERFORMANCE (P95 Latency)\")\n",
    "print(\"-\" * 80)\n",
    "write_ops = stats_df[stats_df['op_category'] == 'write']\n",
    "for operation in ['insert', 'update']:\n",
    "    op_data = write_ops[write_ops['operation'] == operation]\n",
    "    indexed = op_data[op_data['indexed'] == True]['p95_latency_ms_mean'].mean()\n",
    "    no_index = op_data[op_data['indexed'] == False]['p95_latency_ms_mean'].mean()\n",
    "    overhead = (indexed - no_index) / no_index * 100\n",
    "    \n",
    "    print(f\"\\n{operation}:\")\n",
    "    print(f\"  Indexed avg P95:    {indexed:.2f} ms\")\n",
    "    print(f\"  No Index avg P95:   {no_index:.2f} ms\")\n",
    "    print(f\"  Index overhead:     {overhead:+.1f}%\")\n",
    "\n",
    "print(\"\\n\\n3. OVERALL THROUGHPUT\")\n",
    "print(\"-\" * 80)\n",
    "throughput_summary = stats_df.groupby(['indexed', 'ratio_label']).agg({\n",
    "    'total_ops_per_sec_mean': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "for ratio in sorted(throughput_summary['ratio_label'].unique()):\n",
    "    ratio_data = throughput_summary[throughput_summary['ratio_label'] == ratio]\n",
    "    indexed = ratio_data[ratio_data['indexed'] == True]['total_ops_per_sec_mean'].mean()\n",
    "    no_index = ratio_data[ratio_data['indexed'] == False]['total_ops_per_sec_mean'].mean()\n",
    "    change = (indexed - no_index) / no_index * 100\n",
    "    \n",
    "    print(f\"\\nRatio {ratio}:\")\n",
    "    print(f\"  Indexed avg:    {indexed:.2f} ops/sec\")\n",
    "    print(f\"  No Index avg:   {no_index:.2f} ops/sec\")\n",
    "    print(f\"  Change:         {change:+.1f}%\")\n",
    "\n",
    "print(\"\\n\\n4. ERROR RATES\")\n",
    "print(\"-\" * 80)\n",
    "total_ops = df['count'].sum()\n",
    "total_errors = df['error'].sum()\n",
    "error_rate = total_errors / total_ops * 100 if total_ops > 0 else 0\n",
    "\n",
    "print(f\"Total operations:  {total_ops:,}\")\n",
    "print(f\"Total errors:      {total_errors:,}\")\n",
    "print(f\"Error rate:        {error_rate:.4f}%\")\n",
    "\n",
    "if total_errors > 0:\n",
    "    print(\"\\nError breakdown by type:\")\n",
    "    print(f\"  Statement timeouts: {df['timeout_statement'].sum()}\")\n",
    "    print(f\"  Lock timeouts:      {df['timeout_lock'].sum()}\")\n",
    "    print(f\"  Deadlocks:          {df['deadlock'].sum()}\")\n",
    "    print(f\"  Other errors:       {df['error_other'].sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Decision Recommendations\n",
    "\n",
    "Based on the analysis above, generate actionable recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate decision matrix\n",
    "def generate_recommendations(overhead_df, stats_df):\n",
    "    \"\"\"\n",
    "    Generate recommendations based on comprehensive overhead analysis.\n",
    "    \n",
    "    Decision criteria (in priority order):\n",
    "    1. Timeouts: If no-index configuration has timeouts, index is REQUIRED\n",
    "    2. Critical operations: Point lookups and ordered queries must be fast\n",
    "    3. Throughput: Consider overall system throughput improvements\n",
    "    4. Latency: Balance read improvements vs write overhead\n",
    "    \"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    # Get timeout data grouped by configuration\n",
    "    # timeout_data = stats_df\n",
    "    # .groupby(['indexed', 'ratio_label', 'concurrency']).agg({\n",
    "    #     'timeout_statement': 'sum',\n",
    "    #     'timeout_lock': 'sum'\n",
    "    # }).reset_index()\n",
    "    \n",
    "    # Group by ratio and concurrency\n",
    "    for ratio in sorted(overhead_df['ratio_label'].unique()):\n",
    "        for concurrency in sorted(overhead_df['concurrency'].unique()):\n",
    "            subset = overhead_df[\n",
    "                (overhead_df['ratio_label'] == ratio) &\n",
    "                (overhead_df['concurrency'] == concurrency)\n",
    "            ]\n",
    "            \n",
    "            # Get timeout counts for no-index configuration\n",
    "            no_index_timeouts = stats_df[\n",
    "                (stats_df['indexed'] == False) &\n",
    "                (stats_df['ratio_label'] == ratio) &\n",
    "                (stats_df['concurrency'] == concurrency)\n",
    "            ]\n",
    "            total_timeouts = 0\n",
    "            if len(no_index_timeouts) > 0:\n",
    "                total_timeouts = (\n",
    "                    no_index_timeouts['timeout_statement_sum'].sum() + \n",
    "                    no_index_timeouts['timeout_lock_sum'].sum()\n",
    "                )\n",
    "            \n",
    "            # Calculate operation-specific metrics\n",
    "            read_overhead = subset[subset['op_category'] == 'read']['p95_overhead_pct'].mean()\n",
    "            write_overhead = subset[subset['op_category'] == 'write']['p95_overhead_pct'].mean()\n",
    "            \n",
    "            # Get critical operation metrics (point_lookup and range_order)\n",
    "            critical_ops = subset[subset['operation'].isin(['point_lookup', 'range_order'])]\n",
    "            critical_overhead = critical_ops['p95_overhead_pct'].mean() if len(critical_ops) > 0 else 0\n",
    "            \n",
    "            # Get throughput improvement (indexed vs no-index)\n",
    "            throughput_improvement = subset['throughput_overhead_pct'].mean()\n",
    "            \n",
    "            # Decision logic with priority ordering\n",
    "            if total_timeouts > 0:\n",
    "                decision = \"âœ“ Use Index\"\n",
    "                reason = f\"CRITICAL: {total_timeouts} timeouts without index (system unusable)\"\n",
    "                priority = \"P0 - Required\"\n",
    "                \n",
    "            elif critical_overhead < -50:  # Critical ops improve by >50%\n",
    "                decision = \"âœ“ Use Index\"\n",
    "                reason = f\"Critical operations improve by {-critical_overhead:.0f}%\"\n",
    "                priority = \"P1 - Highly Recommended\"\n",
    "                \n",
    "            elif throughput_improvement > 1000:  # >10x throughput improvement\n",
    "                decision = \"âœ“ Use Index\"\n",
    "                reason = f\"Throughput improves by {throughput_improvement:.0f}%\"\n",
    "                priority = \"P1 - Highly Recommended\"\n",
    "                \n",
    "            elif read_overhead < -30 and write_overhead < 200:  # Read improvement outweighs write cost\n",
    "                decision = \"âœ“ Use Index\"\n",
    "                reason = f\"Read latency -{-read_overhead:.0f}% vs write +{write_overhead:.0f}%\"\n",
    "                priority = \"P2 - Recommended\"\n",
    "                \n",
    "            elif write_overhead > 100 and read_overhead > -20:  # High write cost, minimal read benefit\n",
    "                decision = \"âœ— Skip Index\"\n",
    "                reason = f\"High write overhead (+{write_overhead:.0f}%) with minimal read benefit\"\n",
    "                priority = \"P3 - Not Recommended\"\n",
    "                \n",
    "            else:\n",
    "                decision = \"~ Marginal\"\n",
    "                reason = f\"Mixed impact: read {read_overhead:+.0f}%, write {write_overhead:+.0f}%\"\n",
    "                priority = \"P4 - Case-by-case\"\n",
    "            \n",
    "            recommendations.append({\n",
    "                'ratio': ratio,\n",
    "                'concurrency': concurrency,\n",
    "                'read_overhead_pct': read_overhead,\n",
    "                'write_overhead_pct': write_overhead,\n",
    "                'critical_ops_overhead_pct': critical_overhead,\n",
    "                'throughput_improvement_pct': throughput_improvement,\n",
    "                'no_index_timeouts': total_timeouts,\n",
    "                'decision': decision,\n",
    "                'priority': priority,\n",
    "                'reason': reason\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(recommendations)\n",
    "\n",
    "recommendations_df = generate_recommendations(overhead_df, stats_df)\n",
    "\n",
    "print(\"DECISION MATRIX: When to Use Index?\")\n",
    "print(\"=\"*120)\n",
    "print(\"Legend: âœ“ = Use Index | âœ— = Skip Index | ~ = Marginal benefit\")\n",
    "print(\"        P0 = Required | P1 = Highly Recommended | P2 = Recommended | P3 = Not Recommended | P4 = Case-by-case\")\n",
    "print(\"=\"*120)\n",
    "display(recommendations_df.sort_values(['ratio', 'concurrency']))\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"SUMMARY RECOMMENDATION\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "use_index_count = len(recommendations_df[recommendations_df['decision'] == 'âœ“ Use Index'])\n",
    "skip_index_count = len(recommendations_df[recommendations_df['decision'] == 'âœ— Skip Index'])\n",
    "marginal_count = len(recommendations_df[recommendations_df['decision'] == '~ Marginal'])\n",
    "\n",
    "print(f\"\\nOut of {len(recommendations_df)} tested configurations:\")\n",
    "print(f\"  âœ“ Index beneficial:  {use_index_count} ({use_index_count/len(recommendations_df)*100:.1f}%)\")\n",
    "print(f\"  âœ— Index detrimental: {skip_index_count} ({skip_index_count/len(recommendations_df)*100:.1f}%)\")\n",
    "print(f\"  ~ Marginal impact:   {marginal_count} ({marginal_count/len(recommendations_df)*100:.1f}%)\")\n",
    "\n",
    "# Extract patterns by priority\n",
    "print(\"\\nDecision Breakdown by Priority:\")\n",
    "priority_counts = recommendations_df['priority'].value_counts().sort_index()\n",
    "for priority, count in priority_counts.items():\n",
    "    print(f\"  {priority}: {count} configurations\")\n",
    "\n",
    "# Show configurations with timeouts\n",
    "timeout_configs = recommendations_df[recommendations_df['no_index_timeouts'] > 0]\n",
    "if len(timeout_configs) > 0:\n",
    "    print(f\"\\nâš ï¸  CRITICAL: {len(timeout_configs)} configurations have timeouts without index\")\n",
    "    print(\"   These configurations are UNUSABLE without indexing:\")\n",
    "    for _, row in timeout_configs.iterrows():\n",
    "        print(f\"     â€¢ {row['ratio']} ratio, concurrency {row['concurrency']}: {row['no_index_timeouts']} timeouts\")\n",
    "\n",
    "# General patterns for index usage\n",
    "print(\"\\nðŸ“Š General Patterns:\")\n",
    "use_index_configs = recommendations_df[recommendations_df['decision'] == 'âœ“ Use Index']\n",
    "if len(use_index_configs) > 0:\n",
    "    avg_throughput_improvement = use_index_configs['throughput_improvement_pct'].mean()\n",
    "    avg_critical_improvement = use_index_configs['critical_ops_overhead_pct'].mean()\n",
    "    \n",
    "    print(f\"  âœ“ Index recommended for ALL {len(use_index_configs)}/{len(recommendations_df)} disk-resident configurations\")\n",
    "    print(f\"    - Average throughput improvement: {avg_throughput_improvement:,.0f}%\")\n",
    "    print(f\"    - Average critical operation improvement: {-avg_critical_improvement:.1f}%\")\n",
    "    print(f\"    - Configurations with timeouts: {len(timeout_configs)}\")\n",
    "\n",
    "skip_index_configs = recommendations_df[recommendations_df['decision'] == 'âœ— Skip Index']\n",
    "if len(skip_index_configs) > 0:\n",
    "    print(f\"  âœ— Skip index for {len(skip_index_configs)} configurations:\")\n",
    "    for _, row in skip_index_configs.iterrows():\n",
    "        print(f\"    â€¢ {row['ratio']} ratio at concurrency {row['concurrency']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"CONCLUSION FOR DISK-RESIDENT DATASETS (16M rows, ~2Ã— RAM)\")\n",
    "print(\"=\"*120)\n",
    "print(\"\\nðŸŽ¯ For disk-resident workloads with ANY read operations:\")\n",
    "print(\"   â†’ Index is REQUIRED, not optional\")\n",
    "print(\"   â†’ Without index: 2,736 query timeouts across tested configurations\")\n",
    "print(\"   â†’ With index: 100,000%+ throughput improvements\")\n",
    "print(\"\\nðŸ’¡ Write overhead (+50-150%) is acceptable cost for system usability\")\n",
    "print(\"   â†’ Insert P95: 10ms (indexed) vs 4ms (no index) - both fast\")\n",
    "print(\"   â†’ Update P95: 13ms (indexed) vs 7ms (no index) - both fast\")\n",
    "print(\"\\nâš ï¸  NOTE: This analysis covers DISK-RESIDENT data only.\")\n",
    "print(\"   For memory-resident datasets (~0.25Ã— RAM), different tradeoffs may apply.\")\n",
    "print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved Decision Logic\n",
    "\n",
    "The recommendation algorithm uses a **priority-based decision tree** that accounts for:\n",
    "\n",
    "1. **System Usability (P0 - Required)**: Any configuration with timeouts in the no-index case requires indexing\n",
    "2. **Critical Operation Performance (P1 - Highly Recommended)**: Point lookups and range ORDER BY queries must be fast (>50% improvement)\n",
    "3. **Throughput Impact (P1 - Highly Recommended)**: Overall system throughput improvements >1000% justify indexing\n",
    "4. **Balanced Tradeoff (P2 - Recommended)**: Read improvements outweigh write overhead\n",
    "5. **Write-Heavy Penalty (P3 - Not Recommended)**: High write cost with minimal read benefit\n",
    "6. **Marginal Cases (P4 - Case-by-case)**: Mixed results requiring domain-specific decisions\n",
    "\n",
    "This multi-factor approach prevents misleading averages and recognizes that **system failures (timeouts) are not negotiable**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export aggregated results and recommendations\n",
    "output_dir = Path('analysis_outputs')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Export aggregated statistics\n",
    "stats_df.to_csv(output_dir / 'aggregated_statistics.csv', index=False)\n",
    "print(f\"Exported aggregated statistics to {output_dir / 'aggregated_statistics.csv'}\")\n",
    "\n",
    "# Export overhead analysis\n",
    "overhead_df.to_csv(output_dir / 'index_overhead_analysis.csv', index=False)\n",
    "print(f\"Exported overhead analysis to {output_dir / 'index_overhead_analysis.csv'}\")\n",
    "\n",
    "# Export recommendations (with updated decision logic)\n",
    "recommendations_df.to_csv(output_dir / 'recommendations.csv', index=False)\n",
    "print(f\"Exported recommendations to {output_dir / 'recommendations.csv'}\")\n",
    "print(f\"  â†’ Includes timeout counts, priority levels, and improved decision logic\")\n",
    "\n",
    "# Export scalability analysis\n",
    "scalability_df.to_csv(output_dir / 'scalability_analysis.csv', index=False)\n",
    "print(f\"Exported scalability analysis to {output_dir / 'scalability_analysis.csv'}\")\n",
    "\n",
    "print(\"\\nâœ“ All analysis outputs exported successfully!\")\n",
    "\n",
    "# Summary of exported plots\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPORTED PLOTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "plot_files = sorted(PLOTS_DIR.glob('*.png'))\n",
    "if plot_files:\n",
    "    print(f\"\\nTotal plots exported: {len(plot_files)}\")\n",
    "    print(f\"Location: {PLOTS_DIR.absolute()}\\n\")\n",
    "    for i, plot_file in enumerate(plot_files, 1):\n",
    "        size_kb = plot_file.stat().st_size / 1024\n",
    "        print(f\"{i:2d}. {plot_file.name:<45s} ({size_kb:>7.1f} KB)\")\n",
    "else:\n",
    "    print(\"\\nNo plots found. Make sure to run all plotting cells above.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create summary report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nðŸ“ All outputs saved to: {output_dir.absolute()}\")\n",
    "print(f\"\\nDataset: 16M rows (disk-resident, ~2Ã— RAM)\")\n",
    "print(f\"Configurations tested: {len(recommendations_df)}\")\n",
    "print(f\"Total data points: {len(df)} (3 rounds Ã— {len(df)//3} measurements)\")\n",
    "print(f\"\\nâœ“ Index recommended: {len(recommendations_df[recommendations_df['decision'] == 'âœ“ Use Index'])} configurations\")\n",
    "print(f\"âœ— Index not recommended: {len(recommendations_df[recommendations_df['decision'] == 'âœ— Skip Index'])} configurations\")\n",
    "print(f\"~ Marginal benefit: {len(recommendations_df[recommendations_df['decision'] == '~ Marginal'])} configurations\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has analyzed the performance tradeoffs of B-tree indexing across:\n",
    "- **3 read/write ratios**: 90/10, 50/50, 10/90\n",
    "- **3 concurrency levels**: 1, 8, 32 clients\n",
    "- **2 index configurations**: With and without B-tree index\n",
    "- **3 experimental rounds** for statistical confidence\n",
    "\n",
    "### Next Steps:\n",
    "1. Review the decision matrix above to determine optimal index strategy\n",
    "2. Examine specific operation types that benefit most from indexing\n",
    "3. Consider the write overhead when making indexing decisions\n",
    "4. Use exported CSV files for further analysis or reporting\n",
    "\n",
    "For questions or modifications to the analysis, refer to the PROPOSAL.md document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Exported Plots Reference\n",
    "\n",
    "All plots are automatically saved to `analysis_outputs/plots/` as high-resolution PNG files for use in reports and presentations.\n",
    "\n",
    "### Plot Inventory:\n",
    "\n",
    "1. **01_p95_latency_all_operations.png** - P95 latency comparison for all 5 operations across all ratios and concurrency levels\n",
    "2. **02_p95_latency_read_operations.png** - Focused view on read operations (point_lookup, range_scan, range_order)\n",
    "3. **03_p95_latency_write_operations.png** - Focused view on write operations (insert, update)\n",
    "4. **04_total_throughput.png** - Overall system throughput by ratio and concurrency\n",
    "5. **05_per_operation_throughput.png** - Detailed per-operation throughput breakdown\n",
    "6. **06_write_overhead.png** - Write operation overhead analysis showing index impact\n",
    "7. **07_heatmap_p95_overhead.png** - Heatmap visualization of P95 latency overhead by configuration\n",
    "8. **08_heatmap_throughput_overhead.png** - Heatmap visualization of throughput impact by configuration\n",
    "\n",
    "### CSV Exports:\n",
    "\n",
    "Located in `analysis_outputs/`:\n",
    "- **aggregated_statistics.csv** - Full statistical summary with means, std, and 95% CI\n",
    "- **index_overhead_analysis.csv** - Detailed overhead calculations for all configurations\n",
    "- **recommendations.csv** - Decision matrix for index usage\n",
    "- **scalability_analysis.csv** - Concurrency scaling factors (c32/c1 ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bsj240cuieb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check c16 variability vs other concurrency levels\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Filter variability data for analysis\n",
    "c16_variability = variability[variability['concurrency'] == 16].copy()\n",
    "other_variability = variability[variability['concurrency'] != 16].copy()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"C16 VARIABILITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nC16 configurations:\")\n",
    "print(f\"  Count: {len(c16_variability)}\")\n",
    "print(f\"  Median CV: {c16_variability['cv'].median():.2f}%\")\n",
    "print(f\"  Mean CV: {c16_variability['cv'].mean():.2f}%\")\n",
    "print(f\"  Max CV: {c16_variability['cv'].max():.2f}%\")\n",
    "\n",
    "print(f\"\\nOther concurrency levels (c1, c8, c32):\")\n",
    "print(f\"  Count: {len(other_variability)}\")\n",
    "print(f\"  Median CV: {other_variability['cv'].median():.2f}%\")\n",
    "print(f\"  Mean CV: {other_variability['cv'].mean():.2f}%\")\n",
    "print(f\"  Max CV: {other_variability['cv'].max():.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 10 C16 CONFIGURATIONS BY VARIABILITY\")\n",
    "print(\"=\"*80)\n",
    "c16_top = c16_variability.nlargest(10, 'cv')\n",
    "print(c16_top[['indexed', 'ratio_label', 'operation', 'p95_latency_ms_mean', 'p95_latency_ms_std', 'cv']].to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CV DISTRIBUTION BY CONCURRENCY LEVEL\")\n",
    "print(\"=\"*80)\n",
    "cv_by_concurrency = variability.groupby('concurrency')['cv'].agg(['count', 'mean', 'median', 'max'])\n",
    "print(cv_by_concurrency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0rjjqk8jets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check raw values across rounds for problematic c16 configs\n",
    "print(\"=\"*80)\n",
    "print(\"RAW P95 VALUES ACROSS 3 ROUNDS FOR HIGHEST VARIABILITY C16 CONFIGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Focus on the top problematic ones\n",
    "problematic_configs = [\n",
    "    (False, '90/10', 16, 'insert'),\n",
    "    (False, '50/50', 16, 'range_order'),\n",
    "    (False, '50/50', 16, 'point_lookup'),\n",
    "]\n",
    "\n",
    "for indexed, ratio, concurrency, operation in problematic_configs:\n",
    "    raw_data = df[\n",
    "        (df['indexed'] == indexed) &\n",
    "        (df['ratio_label'] == ratio) &\n",
    "        (df['concurrency'] == concurrency) &\n",
    "        (df['operation'] == operation)\n",
    "    ].sort_values('round')\n",
    "    \n",
    "    if len(raw_data) > 0:\n",
    "        print(f\"\\n{operation} - {ratio} ratio - {'Indexed' if indexed else 'No Index'} - c{concurrency}:\")\n",
    "        print(f\"  Round 1: {raw_data.iloc[0]['p95_latency_ms']:.2f} ms\")\n",
    "        print(f\"  Round 2: {raw_data.iloc[1]['p95_latency_ms']:.2f} ms\")\n",
    "        print(f\"  Round 3: {raw_data.iloc[2]['p95_latency_ms']:.2f} ms\")\n",
    "        print(f\"  Mean: {raw_data['p95_latency_ms'].mean():.2f} ms\")\n",
    "        print(f\"  Std: {raw_data['p95_latency_ms'].std():.2f} ms\")\n",
    "        print(f\"  CV: {(raw_data['p95_latency_ms'].std() / raw_data['p95_latency_ms'].mean() * 100):.2f}%\")\n",
    "        \n",
    "        # Check if any round had timeouts\n",
    "        print(f\"  Timeouts: R1={raw_data.iloc[0]['timeout_statement']}, R2={raw_data.iloc[1]['timeout_statement']}, R3={raw_data.iloc[2]['timeout_statement']}\")\n",
    "        print(f\"  Success counts: R1={raw_data.iloc[0]['success']}, R2={raw_data.iloc[1]['success']}, R3={raw_data.iloc[2]['success']}\")\n",
    "\n",
    "# Compare with c32 for same configs\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON WITH C32 (for reference)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for indexed, ratio, _, operation in problematic_configs:\n",
    "    raw_data = df[\n",
    "        (df['indexed'] == indexed) &\n",
    "        (df['ratio_label'] == ratio) &\n",
    "        (df['concurrency'] == 32) &\n",
    "        (df['operation'] == operation)\n",
    "    ].sort_values('round')\n",
    "    \n",
    "    if len(raw_data) > 0:\n",
    "        print(f\"\\n{operation} - {ratio} ratio - {'Indexed' if indexed else 'No Index'} - c32:\")\n",
    "        print(f\"  P95 values: {raw_data.iloc[0]['p95_latency_ms']:.2f}, {raw_data.iloc[1]['p95_latency_ms']:.2f}, {raw_data.iloc[2]['p95_latency_ms']:.2f} ms\")\n",
    "        print(f\"  CV: {(raw_data['p95_latency_ms'].std() / raw_data['p95_latency_ms'].mean() * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tua67lyg1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if high variability is specific to c16 or a general \"No Index\" pattern\n",
    "print(\"=\"*80)\n",
    "print(\"VARIABILITY PATTERN ANALYSIS: INDEXED vs NO INDEX\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Split by indexed status\n",
    "indexed_var = variability[variability['indexed'] == True]['cv']\n",
    "no_index_var = variability[variability['indexed'] == False]['cv']\n",
    "\n",
    "print(f\"\\nIndexed configurations:\")\n",
    "print(f\"  Count: {len(indexed_var)}\")\n",
    "print(f\"  Mean CV: {indexed_var.mean():.2f}%\")\n",
    "print(f\"  Median CV: {indexed_var.median():.2f}%\")\n",
    "print(f\"  Max CV: {indexed_var.max():.2f}%\")\n",
    "print(f\"  % with CV > 10%: {(indexed_var > 10).sum() / len(indexed_var) * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\nNo Index configurations:\")\n",
    "print(f\"  Count: {len(no_index_var)}\")\n",
    "print(f\"  Mean CV: {no_index_var.mean():.2f}%\")\n",
    "print(f\"  Median CV: {no_index_var.median():.2f}%\")\n",
    "print(f\"  Max CV: {no_index_var.max():.2f}%\")\n",
    "print(f\"  % with CV > 10%: {(no_index_var > 10).sum() / len(no_index_var) * 100:.1f}%\")\n",
    "\n",
    "# Check by concurrency and indexed status\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CV BY CONCURRENCY AND INDEX STATUS\")\n",
    "print(\"=\"*80)\n",
    "cv_table = variability.groupby(['concurrency', 'indexed'])['cv'].agg(['mean', 'median', 'max', 'count'])\n",
    "print(cv_table)\n",
    "\n",
    "# Find all configs with CV > 20%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL CONFIGURATIONS WITH CV > 20%\")\n",
    "print(\"=\"*80)\n",
    "high_cv = variability[variability['cv'] > 20].sort_values('cv', ascending=False)\n",
    "print(f\"\\nTotal: {len(high_cv)} configurations\")\n",
    "print(high_cv[['indexed', 'ratio_label', 'concurrency', 'operation', 'p95_latency_ms_mean', 'cv']].to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
